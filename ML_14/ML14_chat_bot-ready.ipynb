{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Система и среда выполнения кода"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import wmi\n",
    "\n",
    "computer = wmi.WMI()\n",
    "computer_info = computer.Win32_ComputerSystem()[0]\n",
    "os_info = computer.Win32_OperatingSystem()[0]\n",
    "proc_info = computer.Win32_Processor()[0]\n",
    "gpu_info = computer.Win32_VideoController()[0]\n",
    "\n",
    "os_name = os_info.Name.encode('utf-8').split(b'|')[0]\n",
    "os_version = ' '.join([os_info.Version, os_info.BuildNumber])\n",
    "system_ram = float(os_info.TotalVisibleMemorySize) / 1048576  # KB to GB\n",
    "\n",
    "print('OS Name: {0}'.format(os_name))\n",
    "print('OS Version: {0}'.format(os_version))\n",
    "print('CPU: {0}'.format(proc_info.Name))\n",
    "print('RAM: {0} GB'.format(system_ram))\n",
    "print('Graphics Card: {0}'.format(gpu_info.Name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### OS Name: 'Microsoft Windows 7 64'\n",
    "###### OS Version: 6.1.7601 7601\n",
    "###### CPU: Intel(R) Core(TM) i5-4300M CPU @ 2.60GHz\n",
    "###### RAM: 7.907649993896484 GB\n",
    "###### Graphics Card: Intel(R) HD Graphics 4600 /n\n",
    "\n",
    "#### *** Внимание! Этот ноутбук выполняется на PC с вышеуказанными параметрами около 3 часов.\n",
    "\n",
    "Можно сразу перейти к результатам работы.\n",
    "\n",
    "Для этого необходимо:\n",
    "    \n",
    "1. Скачать полученные в ходе работы этого ноутбука файлы по ссылке: \n",
    "    \n",
    "https://drive.google.com/file/d/1NKYEAx9B5YgfHKrbT-ndTxQPgQJpYQTl/view?usp=sharing\n",
    "    \n",
    "2. Распаковать в любой  каталог. Указать этот каталог в переменной 'DATA_PATH' (в параграфе с imports)\n",
    "\n",
    "3. Запустить параграф с imports и параграф с функцией get_answer()\n",
    "\n",
    "\n",
    "!!! При 'честном' запуске ноутбука следующие файлы должны находиться в каталоге с именем содержащемся в 'DATA_PATH' :\n",
    "1. 'ProductsDataset.csv'\n",
    "2. 'Otvety.txt'\n",
    "3. 'prepared_answers.txt' (должен быть заранее подготовлен или же потребуется запустить код для его получения в пункте 1.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acapture==1.2.6\n",
      "aiohttp==3.8.0\n",
      "aiosignal==1.2.0\n",
      "alabaster==0.7.12\n",
      "anaconda-client==1.7.2\n",
      "anaconda-navigator==1.9.12\n",
      "anaconda-project==0.8.3\n",
      "annoy==1.17.0\n",
      "app==0.0.1\n",
      "appdirs==1.4.4\n",
      "argh==0.26.2\n",
      "asn1crypto==1.3.0\n",
      "astroid==2.3.3\n",
      "astropy==4.0\n",
      "astunparse==1.6.3\n",
      "async-generator==1.10\n",
      "async-timeout==4.0.1\n",
      "asynctest==0.13.0\n",
      "atomicwrites==1.3.0\n",
      "attrs==19.3.0\n",
      "autopep8==1.4.4\n",
      "Babel==2.8.0\n",
      "backcall==0.1.0\n",
      "backports.functools-lru-cache==1.6.1\n",
      "backports.shutil-get-terminal-size==1.0.0\n",
      "backports.tempfile==1.0\n",
      "backports.weakref==1.0.post1\n",
      "bcrypt==3.1.7\n",
      "beautifulsoup4==4.8.2\n",
      "bitarray==1.2.1\n",
      "bkcharts==0.2\n",
      "bleach==3.1.0\n",
      "blinker==1.4\n",
      "bokeh==1.4.0\n",
      "boto==2.49.0\n",
      "Bottleneck==1.3.2\n",
      "browser-cookie3==0.11.4\n",
      "bs4==0.0.1\n",
      "cachetools==4.2.2\n",
      "catboost==0.24.4\n",
      "certifi==2020.12.5\n",
      "cffi==1.14.0\n",
      "chardet==3.0.4\n",
      "charset-normalizer==2.0.7\n",
      "Click==7.0\n",
      "cloudpickle==1.3.0\n",
      "clyent==1.2.2\n",
      "colorama==0.4.3\n",
      "colorlover==0.3.0\n",
      "colour==0.1.5\n",
      "comtypes==1.1.7\n",
      "conda==4.8.5\n",
      "conda-build==3.18.11\n",
      "conda-package-handling==1.6.0\n",
      "conda-verify==3.4.2\n",
      "configparser==5.0.0\n",
      "contextlib2==0.6.0.post1\n",
      "cryptography==3.4.7\n",
      "cssselect==1.1.0\n",
      "cufflinks==0.17.3\n",
      "cycler==0.10.0\n",
      "Cython==0.29.23\n",
      "cytoolz==0.10.1\n",
      "dask==2.11.0\n",
      "datasets==1.15.1\n",
      "DAWG-Python==0.7.2\n",
      "decorator==4.4.1\n",
      "defusedxml==0.6.0\n",
      "diff-match-patch==20181111\n",
      "dill==0.3.4\n",
      "distributed==2.11.0\n",
      "docker-pycreds==0.4.0\n",
      "docopt==0.6.2\n",
      "docutils==0.16\n",
      "dtreeviz==1.1.4\n",
      "easydict==1.9\n",
      "entrypoints==0.3\n",
      "et-xmlfile==1.0.1\n",
      "fake-useragent==0.1.11\n",
      "fastcache==1.1.0\n",
      "filelock==3.0.12\n",
      "flake8==3.7.9\n",
      "Flask==1.1.1\n",
      "Flask-Login==0.5.0\n",
      "frozenlist==1.2.0\n",
      "fsspec==2021.11.0\n",
      "future==0.18.2\n",
      "gensim==4.1.2\n",
      "geojson==2.5.0\n",
      "gevent==1.4.0\n",
      "gitdb==4.0.9\n",
      "GitPython==3.1.24\n",
      "glob2==0.7\n",
      "google-auth==1.30.0\n",
      "graphviz==0.16\n",
      "greenlet==0.4.15\n",
      "h11==0.12.0\n",
      "h2==4.0.0\n",
      "h5py==2.10.0\n",
      "HeapDict==1.0.1\n",
      "hpack==4.0.0\n",
      "html5lib==1.0.1\n",
      "huggingface-hub==0.1.2\n",
      "hyperframe==6.0.1\n",
      "hypothesis==5.5.4\n",
      "idna==2.8\n",
      "imageio==2.6.1\n",
      "imagesize==1.2.0\n",
      "imbalanced-learn==0.7.0\n",
      "imblearn==0.0\n",
      "importlib-metadata==2.1.1\n",
      "intervaltree==3.0.2\n",
      "investpy==1.0\n",
      "ipykernel==5.1.4\n",
      "ipython==7.12.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.5.1\n",
      "isort==4.3.21\n",
      "itsdangerous==1.1.0\n",
      "jdcal==1.4.1\n",
      "jedi==0.14.1\n",
      "Jinja2==2.11.1\n",
      "joblib==0.14.1\n",
      "json5==0.9.1\n",
      "jsonschema==3.2.0\n",
      "jupyter==1.0.0\n",
      "jupyter-client==5.3.4\n",
      "jupyter-console==6.1.0\n",
      "jupyter-contrib-core==0.3.3\n",
      "jupyter-contrib-nbextensions==0.5.1\n",
      "jupyter-core==4.6.1\n",
      "jupyter-highlight-selected-word==0.2.0\n",
      "jupyter-latex-envs==1.4.6\n",
      "jupyter-nbextensions-configurator==0.4.1\n",
      "jupyterlab==1.2.6\n",
      "jupyterlab-server==1.0.6\n",
      "jwt==1.2.0\n",
      "kaitaistruct==0.9\n",
      "Keras==2.4.3\n",
      "keyring==21.1.0\n",
      "kiwisolver==1.1.0\n",
      "lazy-object-proxy==1.4.3\n",
      "libarchive-c==2.8\n",
      "lightgbm==3.1.1\n",
      "llvmlite==0.31.0\n",
      "locket==0.2.0\n",
      "lxml==4.5.0\n",
      "lz4==3.1.1\n",
      "Markdown==3.3.3\n",
      "MarkupSafe==1.1.1\n",
      "matplotlib==3.4.3\n",
      "mccabe==0.6.1\n",
      "menuinst==1.4.16\n",
      "mistune==0.8.4\n",
      "mkl-fft==1.0.15\n",
      "mkl-random==1.1.0\n",
      "mkl-service==2.3.0\n",
      "mock==4.0.1\n",
      "more-itertools==8.2.0\n",
      "mpmath==1.1.0\n",
      "msgpack==0.6.1\n",
      "mss==6.0.0\n",
      "multidict==5.2.0\n",
      "multipledispatch==0.6.0\n",
      "multiprocess==0.70.12.2\n",
      "navigator-updater==0.2.1\n",
      "nbconvert==5.6.1\n",
      "nbformat==5.0.4\n",
      "nest-asyncio==1.5.1\n",
      "networkx==2.4\n",
      "nltk==3.4.5\n",
      "nose==1.3.7\n",
      "notebook==6.0.3\n",
      "numba==0.48.0\n",
      "numexpr==2.7.1\n",
      "numpy==1.19.3\n",
      "numpydoc==0.9.2\n",
      "oauthlib==3.1.0\n",
      "olefile==0.46\n",
      "opencv-python==4.4.0.44\n",
      "openpyxl==3.0.3\n",
      "outcome==1.1.0\n",
      "packaging==21.2\n",
      "pandas==1.0.1\n",
      "pandocfilters==1.4.2\n",
      "paramiko==2.7.1\n",
      "parse==1.19.0\n",
      "parso==0.5.2\n",
      "partd==1.1.0\n",
      "path==13.1.0\n",
      "pathlib2==2.3.5\n",
      "pathtools==0.1.2\n",
      "patsy==0.5.1\n",
      "pbkdf2==1.3\n",
      "pep8==1.7.1\n",
      "pexpect==4.8.0\n",
      "phantomjs==1.4.1\n",
      "pickleshare==0.7.5\n",
      "Pillow==7.0.0\n",
      "pixiedust==1.1.19\n",
      "pkginfo==1.5.0.1\n",
      "plotly==4.9.0\n",
      "pluggy==0.13.1\n",
      "ply==3.11\n",
      "prometheus-client==0.7.1\n",
      "promise==2.3\n",
      "prompt-toolkit==3.0.3\n",
      "protobuf==3.16.0\n",
      "psutil==5.6.7\n",
      "py==1.8.1\n",
      "pyaes==1.6.1\n",
      "pyarrow==6.0.0\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycodestyle==2.5.0\n",
      "pycosat==0.6.3\n",
      "pycparser==2.19\n",
      "pycrypto==2.6.1\n",
      "pycryptodome==3.9.9\n",
      "pycurl==7.43.0.5\n",
      "pydivert==2.1.0\n",
      "pydocstyle==4.0.1\n",
      "pydotplus==2.0.2\n",
      "pyee==8.1.0\n",
      "pyflakes==2.1.1\n",
      "pygame==1.9.6\n",
      "Pygments==2.5.2\n",
      "pylint==2.4.4\n",
      "pymorphy2==0.9.1\n",
      "pymorphy2-dicts-ru==2.4.417127.4579844\n",
      "PyMySQL==1.0.2\n",
      "pymystem3==0.2.0\n",
      "PyNaCl==1.3.0\n",
      "pyodbc===4.0.0-unsupported\n",
      "pyOpenSSL==19.1.0\n",
      "pyparsing==2.4.6\n",
      "pypiwin32==223\n",
      "pyppeteer==0.2.5\n",
      "pyquery==1.4.3\n",
      "pyreadline==2.1\n",
      "pyrsistent==0.15.7\n",
      "PySocks==1.7.1\n",
      "pytest==5.3.5\n",
      "pytest-arraydiff==0.3\n",
      "pytest-astropy==0.8.0\n",
      "pytest-astropy-header==0.1.2\n",
      "pytest-doctestplus==0.5.0\n",
      "pytest-openfiles==0.4.0\n",
      "pytest-remotedata==0.3.2\n",
      "python-dateutil==2.8.1\n",
      "python-jsonrpc-server==0.3.4\n",
      "python-language-server==0.31.7\n",
      "pytz==2019.3\n",
      "PyWavelets==1.1.1\n",
      "pywin32==227\n",
      "pywin32-ctypes==0.2.0\n",
      "pywinpty==0.5.7\n",
      "PyYAML==5.3\n",
      "pyzmq==18.1.1\n",
      "QDarkStyle==2.8\n",
      "QtAwesome==0.6.1\n",
      "qtconsole==4.6.0\n",
      "QtPy==1.9.0\n",
      "requests==2.26.0\n",
      "requests-futures==1.0.0\n",
      "requests-html==0.10.0\n",
      "requests-oauthlib==1.3.0\n",
      "retrying==1.3.3\n",
      "rope==0.16.0\n",
      "rsa==4.7.2\n",
      "Rtree==0.9.3\n",
      "ruamel-yaml==0.15.87\n",
      "scikit-image==0.16.2\n",
      "scikit-learn==0.24.1\n",
      "scipy==1.4.1\n",
      "seaborn==0.11.1\n",
      "selenium==4.0.0\n",
      "selenium-wire==4.2.4\n",
      "Send2Trash==1.5.0\n",
      "sentry-sdk==1.4.3\n",
      "shortuuid==1.0.8\n",
      "simplegeneric==0.8.1\n",
      "singledispatch==3.4.0.3\n",
      "six==1.15.0\n",
      "smart-open==5.2.1\n",
      "smmap==5.0.0\n",
      "sniffio==1.2.0\n",
      "snowballstemmer==2.0.0\n",
      "sortedcollections==1.1.2\n",
      "sortedcontainers==2.1.0\n",
      "soupsieve==1.9.5\n",
      "Sphinx==2.4.0\n",
      "sphinxcontrib-applehelp==1.0.1\n",
      "sphinxcontrib-devhelp==1.0.1\n",
      "sphinxcontrib-htmlhelp==1.0.2\n",
      "sphinxcontrib-jsmath==1.0.1\n",
      "sphinxcontrib-qthelp==1.0.2\n",
      "sphinxcontrib-serializinghtml==1.1.3\n",
      "sphinxcontrib-websupport==1.2.0\n",
      "spyder==4.0.1\n",
      "spyder-kernels==1.8.1\n",
      "SQLAlchemy==1.3.13\n",
      "statsmodels==0.11.0\n",
      "stop-words==2018.7.23\n",
      "stopwords==1.0.0\n",
      "subprocess32==3.5.4\n",
      "sympy==1.5.1\n",
      "tables==3.6.1\n",
      "tblib==1.6.0\n",
      "tensorboard-data-server==0.6.1\n",
      "tensorboard-plugin-wit==1.8.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.8.3\n",
      "testpath==0.4.4\n",
      "threadpoolctl==2.1.0\n",
      "tokenizers==0.10.0rc1\n",
      "toolz==0.10.0\n",
      "tornado==6.0.3\n",
      "tqdm==4.62.3\n",
      "traitlets==4.3.3\n",
      "trio==0.19.0\n",
      "trio-websocket==0.9.2\n",
      "typing-extensions==3.10.0.2\n",
      "ujson==1.35\n",
      "unicodecsv==0.14.1\n",
      "Unidecode==1.1.2\n",
      "urllib3==1.26.7\n",
      "w3lib==1.22.0\n",
      "wandb==0.12.6\n",
      "watchdog==0.10.2\n",
      "wcwidth==0.1.8\n",
      "webencodings==0.5.1\n",
      "websockets==8.1\n",
      "Werkzeug==1.0.0\n",
      "widgetsnbextension==3.5.1\n",
      "win-inet-pton==1.1.0\n",
      "win-unicode-console==0.5\n",
      "wincertstore==0.2\n",
      "WMI==1.5.1\n",
      "wrapt==1.11.2\n",
      "wsproto==1.0.0\n",
      "xgboost==1.3.3\n",
      "xlrd==1.2.0\n",
      "XlsxWriter==1.2.7\n",
      "xlwings==0.17.1\n",
      "xlwt==1.3.0\n",
      "xmljson==0.2.1\n",
      "xmltodict==0.12.0\n",
      "xxhash==2.0.2\n",
      "yahooquery==2.2.8\n",
      "yapf==0.28.0\n",
      "yarl==1.7.2\n",
      "yaspin==2.1.0\n",
      "ze2nb==1.0.0\n",
      "zict==1.0.0\n",
      "zipp==2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could not generate requirement for distribution -umpy 1.18.1 (c:\\users\\dell\\anaconda3\\lib\\site-packages): Parse error at \"'-umpy==1'\": Expected W:(abcd...)\n",
      "WARNING: Could not generate requirement for distribution -atplotlib 3.1.3 (c:\\users\\dell\\anaconda3\\lib\\site-packages): Parse error at \"'-atplotl'\": Expected W:(abcd...)\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import string\n",
    "import os\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import annoy\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "\n",
    "DATA_PATH = 'e:/ML14/DN/chatbot_files/'\n",
    "\n",
    "tqdm.pandas()  # enable 'progress_apply' function\n",
    "\n",
    "tbeg = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Обучение классификатора продуктовых запросов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Создаем датасет 'table' для классификатора. Он состоит из двух датасетов - один (df_products) с текстом содержащим описания или названия продуктов и размеченный как True, а другой (df_questions), содержащий вопросы из болталки и размеченный как False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* чтобы не повторять при отладке длительные процедуры кода - промежуточные результаты сохранялись в файлы, а код в ноутбке сохранялся как NBCcontent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Создание датасета"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Предобработаем ответы mail.ru из файла: к каждому вопросу присоединим 1 ответ и запишем в файл на будущее. \n",
    "# Это позволит нам сэкономить время и ресурсы при дальнейшем препроцессинге текста\n",
    "\n",
    "question = None\n",
    "written = False\n",
    "\n",
    "# Мы идем по всем записям, берем первую строку как вопрос\n",
    "# и после знака --- находим ответ\n",
    "with codecs.open(DATA_PATH + \"prepared_answers.txt\", \"w\", \"utf_8_sig\") as fout:\n",
    "    with open(DATA_PATH + \"Otvety.txt\", \"r\") as fin:\n",
    "        for line in tqdm(fin):\n",
    "            if line.startswith(\"---\"):\n",
    "                written = False\n",
    "                continue\n",
    "            if not written and question is not None:\n",
    "                fout.write(question.replace(\"\\t\", \" \").strip() + \"\\t\" + line.replace(\"\\t\", \" \"))\n",
    "                written = True\n",
    "                question = None\n",
    "                continue\n",
    "            if not written:\n",
    "                question = line.strip()\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e292e8483874d08a848da272c67c009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Так как строки иногда разрываются (по символу \\n) не после последнего поля), то приходится\n",
    "# преобразовать файл 'ProductsDataset.csv' так, чтобы строка заканчивалась после последовательности \".jpg\"\n",
    "# В итоге получаем новый файл 'prodDS.csv'.\n",
    "\n",
    "\n",
    "out_line = ''\n",
    "c = 0\n",
    "with codecs.open(DATA_PATH + \"prodDS.csv\", \"w\", \"utf_8_sig\") as fout:\n",
    "    with codecs.open(DATA_PATH + \"ProductsDataset.csv\", \"r\", \"utf_8_sig\") as fin:\n",
    "        for line in tqdm(fin):\n",
    "            c += 1\n",
    "            # print(c)\n",
    "            # if c>10:\n",
    "            #   break\n",
    "            if c > 1:\n",
    "                if line.endswith(\".jpg\\n\") != True:\n",
    "                    # print('No')\n",
    "                    # print(line)\n",
    "                    not_EOL = True\n",
    "                    out_line = (out_line+line).replace('\\n', ' ')\n",
    "                    continue\n",
    "                else:\n",
    "                    # print('Yes')\n",
    "                    out_line = out_line+line\n",
    "                    # print(out_line)\n",
    "                    fout.write(out_line)\n",
    "                    not_EOL = False\n",
    "                    out_line = ''\n",
    "            else:\n",
    "                fout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH + \"prodDS.csv\")\n",
    "df = df.fillna('')\n",
    "df['label'] = 0  # Добавим разметку\n",
    "df.columns = ['title', 'description', 'product_id', 'category_id', 'subcategory_id',\n",
    "              'properties', 'image_links', 'label']  # исправим 'descrirption' -> 'description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединим названия и описания в один текстовый столбец так, как:\n",
    "# \"...продуктовым можно считать запрос, который равен названию или описанию товара\" .\n",
    "\n",
    "df1 = df[['description', 'label']]\n",
    "df2 = df[['title', 'label']]\n",
    "df1.columns = ['text', 'label']\n",
    "df2.columns = ['text', 'label']\n",
    "\n",
    "df_products = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65a033d1de249a9835de7ac1285cea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Соберем датафрейм с вопросами из болталки\n",
    "questions = []\n",
    "with codecs.open(DATA_PATH + \"prepared_answers.txt\", \"r\", \"utf_8_sig\") as f:\n",
    "    for line in tqdm(f):\n",
    "        spls = line.split(\"\\t\")\n",
    "        questions.append(spls[0])\n",
    "\n",
    "df_questions = pd.DataFrame({'text': questions})\n",
    "df_questions['label'] = 1  # Добавим разметку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем итоговый датасет\n",
    "table = pd.concat([df_products, df_questions], ignore_index=True)\n",
    "# Remove rows with empty text cells\n",
    "table.drop(index=table[table.text == ''].index, inplace=True)\n",
    "table.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "table.to_csv(DATA_PATH + 'ml_14_classify.csv',  encoding = 'utf-8 sig', index = False) # сохраним, чтоб при перезапусках не собирать заново\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Подгружаем датасет (предварительно сохраненный)\n",
    "table = pd.read_csv(DATA_PATH + 'ml_14_classify.csv', encoding='utf-8 sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Новая, не носили ни разу. В реале красивей чем...\n",
       "1          Новые,привезены из Чехии ,указан размер 40,но ...\n",
       "2          Размер 40-42. Брюки почти новые - не знаю как ...\n",
       "3          Продам шапки,кажда 200р.Розовая и белая проданны.\n",
       "4          Темно-синяя, 42 размер,состояние отличное,как ...\n",
       "                                 ...                        \n",
       "1232604    Между словами ПРЕЗИДЕНТ и РЕЗИДЕНТ есть что ли...\n",
       "1232605    \"Если это мое, то оно никуда от меня не денетс...\n",
       "1232606          А Вы халяву любите или совесть имеете???) .\n",
       "1232607    Так много разных гороскопов кто-нибудь может п...\n",
       "1232608    В пылу страстей она сломала мне вставную челюс...\n",
       "Name: text, Length: 1232609, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Препроцессинг датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = RussianStemmer()\n",
    "\n",
    "sw = set(get_stop_words(\"ru\")).union(\n",
    "    set('1234567890qwertyuiopasdfghjklzxcvbnmйцукенгшщзхъфываепнроглшщджэячсмитьбю'))\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "\n",
    "def pprocess(line):\n",
    "    line = re.sub(\"[\\x0b\\x0c\\s]\", ' ', line.lower())\n",
    "    line = re.sub(\"(б\\.у)|(б\\/у)|(б\\\\у)\", 'бу', line)\n",
    "    line = re.sub(\"[^а-яё \\d\\w]\", ' ', line)\n",
    "\n",
    "    spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "    spls = [r.stem(i) for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "    return ' '.join(spls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12f1713ec60461785382370e11096b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1232609 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed, s: 1616.8297770210002\n"
     ]
    }
   ],
   "source": [
    "t = time.perf_counter()\n",
    "table_processed = table['text'].progress_apply(lambda x: pprocess(x))\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# сохраним, чтоб при перезапусках не повторять заново\n",
    "table_processed.to_csv(DATA_PATH + 'ml_14_classify_pprocessed.csv',\n",
    "                       encoding='utf-8 sig', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Подгружаем предобработанный стеммингом датасет (предварительно сохраненный)\n",
    "table_processed = pd.read_csv(\n",
    "    DATA_PATH + 'ml_14_classify_pprocessed.csv', encoding='utf-8 sig')['text'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Разбиение датасета и обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product(69085)/Non-product(1163524) = 0.05937565533671845\n"
     ]
    }
   ],
   "source": [
    "X = table_processed.to_list()\n",
    "y = table['label'].to_list()\n",
    "\n",
    "plt.hist(y)\n",
    "plt.show()\n",
    "s1 = (table['label'] == 1).sum()\n",
    "s0 = (table['label'] == 0).sum()\n",
    "print(f'Product({s0})/Non-product({s1}) = {s0/s1}')\n",
    "# Классы распределены неравномерно - будем использовать stratify при разбиении на тест и трейн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make datasets with similar classes distribution\n",
    "TX_train, TX_test, ly_train, ly_test, text_train, text_test = train_test_split(\n",
    "    X, y, table['text'], test_size=0.20, random_state=42, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed, s: 164.19502953200026\n"
     ]
    }
   ],
   "source": [
    "t = time.perf_counter()\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=50000)\n",
    "vectorizer.fit(TX_train)\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Сохраняем обученный векторайзер\n",
    "with open(DATA_PATH + 'TfidfVectorizer.pickle', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Подгружаем ранее сохраненный и обученный векторайзер\n",
    "with open(DATA_PATH + 'TfidfVectorizer.pickle', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed, s: 68.66169596200052\n"
     ]
    }
   ],
   "source": [
    "t = time.perf_counter()\n",
    "\n",
    "X_train = vectorizer.transform(TX_train)\n",
    "y_train = np.array(ly_train)\n",
    "\n",
    "X_test = vectorizer.transform(TX_test)\n",
    "y_test = np.array(ly_test)\n",
    "\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<986087x50000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12762556 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на текстовые векторы...\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create LinearSVM model object and fit the model\n",
    "model_svm = svm.LinearSVC()\n",
    "model_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем классификатор\n",
    "with open(DATA_PATH + 'query_model_stem_tdfv_best.pickle', 'wb') as f:\n",
    "    pickle.dump(model_svm, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(DATA_PATH + 'query_model_stem_tdfv_best.pickle', 'rb') as f:\n",
    "    model_svm = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9947306934066736 0.9972112315021351 0.9977085896528259\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на различные рекомендуемые метрики при дисбалансе классов\n",
    "predictions = model_svm.predict(X_test)\n",
    "f1_s = f1_score(y_test, predictions)\n",
    "f2 = fbeta_score(y_test, predictions, beta=2.0)\n",
    "accuracy = (predictions == y_test).mean()\n",
    "print(accuracy, f1_s, f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confusion_matrix\n",
    "\n",
    "TP FN\n",
    "\n",
    "FP TN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего запросов в тестовой выборке: 246522\n",
      "Правильно классифицированных продуктовых запросов: 12974 Ошибочно потерянных продуктовых запросов: 843\n",
      "Ошибочно принятых за продуктовые запросы: 456 Правильно классифицированных мусорных запросов: 232249\n",
      "Процент потерянных истинных продуктовых запросов: 6.497610605827039\n",
      "Процент принятых мусорных запросов: 0.19634099608609726\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = y_test\n",
    "y_pred = predictions\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print('Всего запросов в тестовой выборке:', len(y_test))\n",
    "print('Правильно классифицированных продуктовых запросов:',\n",
    "      cm[0, 0], 'Ошибочно потерянных продуктовых запросов:', cm[0, 1])\n",
    "print('Ошибочно принятых за продуктовые запросы:',\n",
    "      cm[1, 0], 'Правильно классифицированных мусорных запросов:', cm[1, 1])\n",
    "print('Процент потерянных истинных продуктовых запросов:',\n",
    "      100*cm[0, 1]/cm[0, 0])\n",
    "print('Процент принятых мусорных запросов:', 100*cm[1, 0]/cm[1, 1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# можно поисследовать FN и FP предсказания\n",
    "adf = pd.DataFrame({'text_test':text_test, 'y_test':y_test, 'predictions':predictions})\n",
    "adf[(adf.y_test == 0) & (adf.predictions == 1)].sort_values('text_test').to_excel('Потерянные истинные продуктовые запросы.xlsx') \n",
    "adf[(adf.y_test == 1) & (adf.predictions == 0)].sort_values('text_test').to_excel('Принятые мусорные запросы.xlsx') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Поиск похожих товаров в контентной части бота"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Препроцессинг и векторизация продуктовых названий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(get_stop_words(\"ru\")).union(\n",
    "    set('1234567890qwertyuiopasdfghjklzxcvbnmйцукенгшщзхъфываепнроглшщджэячсмитьбю'))\n",
    "exclude = set(string.punctuation)\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "\n",
    "def products_preprocess_txt(line):\n",
    "    line = re.sub(\"[\\x0b\\x0c\\s]\", ' ', line.lower())\n",
    "    line = re.sub(\"(б\\.у)|(б\\/у)|(б\\\\у)\", 'бу', line)\n",
    "    line = re.sub(\"[^а-яё \\d\\w]\", ' ', line)\n",
    "\n",
    "    spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "    return spls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для обучения Word2Vec создадим датасет из названий и описаний длиной больше 20 символов.\n",
    "df_products = pd.concat(\n",
    "    [df1.loc[df1.text.str.len() > 20], df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca312608b544885884462c8bd985e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed, s: 237.08028802599983\n"
     ]
    }
   ],
   "source": [
    "t = time.perf_counter()\n",
    "products_processed = df_products['text'].progress_apply(\n",
    "    lambda x: products_preprocess_txt(x))\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# сохраним, чтоб при перезапусках не повторять заново\n",
    "products_processed.to_csv(DATA_PATH + 'ml_14_classify_products.csv',\n",
    "                          encoding='utf-8 sig', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Подгружаем предобработанный датасет (предварительно сохраненный)\n",
    "\n",
    "products_processed = pd.read_csv(\n",
    "    DATA_PATH + 'ml_14_classify_products.csv', encoding='utf-8 sig')['text']\n",
    "\n",
    "\n",
    "def stringToList(string):\n",
    "\n",
    "    string = string[2:len(string)-2]\n",
    "    try:\n",
    "        if len(string) != 0:\n",
    "            newList = string.split(\"', '\")\n",
    "        else:\n",
    "            newList = []\n",
    "    except:\n",
    "        newList = string\n",
    "    return(newList)\n",
    "\n",
    "\n",
    "products_processed = products_processed.progress_apply(\n",
    "    lambda x: stringToList(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed, s: 4.31583441300063\n"
     ]
    }
   ],
   "source": [
    "# Обучаем модель для продуктовых запросов\n",
    "t = time.perf_counter()\n",
    "model_w2v_products = Word2Vec(\n",
    "    sentences=products_processed.to_list(), vector_size=100, min_count=1, window=5)\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Сохраняем модель модель для продуктовых запросов\n",
    "model_w2v_products.save(DATA_PATH + \"w2v_model_products\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Подгружаем предварительно обученную модель\n",
    "model_w2v_products = Word2Vec.load(DATA_PATH + \"w2v_model_products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed, s: 28.512096751000172\n"
     ]
    }
   ],
   "source": [
    "# Складываем в индекс все наши названия продуктовых запросов\n",
    "t = time.perf_counter()\n",
    "index = annoy.AnnoyIndex(100, 'angular')\n",
    "\n",
    "index_map = {}\n",
    "counter = 0\n",
    "\n",
    "\n",
    "def annoy_indexing(row):\n",
    "    global counter\n",
    "    n_w2v = 0\n",
    "    index_map[counter] = row['product_id'] + ' ' + row['title']\n",
    "    product_query = products_preprocess_txt(row['title'])\n",
    "    vector = np.zeros(100)\n",
    "    for word in product_query:\n",
    "        if word in model_w2v_products.wv:\n",
    "            vector += model_w2v_products.wv[word]\n",
    "            n_w2v += 1\n",
    "    if n_w2v > 0:\n",
    "        vector = vector / n_w2v\n",
    "    index.add_item(counter, vector)\n",
    "    counter += 1\n",
    "\n",
    "df.apply(lambda x: annoy_indexing(x), 1)\n",
    "index.build(10)\n",
    "\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Сохраняем индекс продуктовых названий\n",
    "with open(DATA_PATH + 'index_map_products.pickle', 'wb') as f:  # Save it for future use\n",
    "    pickle.dump(index_map, f)\n",
    "index.save(DATA_PATH + 'products.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_description(product_title):\n",
    "    preprocessed_title = products_preprocess_txt(product_title)\n",
    "    n_w2v = 0\n",
    "    vector = np.zeros(100)\n",
    "    for word in preprocessed_title:\n",
    "        if word in model_w2v_products.wv:\n",
    "            vector += model_w2v_products.wv[word]\n",
    "            n_w2v += 1\n",
    "    if n_w2v > 0:\n",
    "        vector = vector / n_w2v\n",
    "    answer_index = index.get_nns_by_vector(vector, 1)\n",
    "    return index_map[answer_index[0]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Подгружаем предварительно подготовленный и сохранненый индекс\n",
    "with open(DATA_PATH + 'index_map_products.pickle', 'rb') as f:\n",
    "    index_map = pickle.load(f)\n",
    "index = annoy.AnnoyIndex(100, 'angular')    \n",
    "index.load(DATA_PATH + 'products.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'58e3cfe6132ca50e053f5f82 Юбка детская ORBY'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_description('Юбка детская ORBY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Болталка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Препроцессинг и векторизация вопросов болталки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpher = MorphAnalyzer()\n",
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "\n",
    "def preprocess_txt(line):\n",
    "    spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "    return spls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f3fee8b18647b089c0d532d7b94414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed, s: 2758.891974395\n"
     ]
    }
   ],
   "source": [
    "t = time.perf_counter()\n",
    "sentences = []\n",
    "c = 0\n",
    "with codecs.open(DATA_PATH + \"Otvety.txt\", \"r\", \"utf_8_sig\") as fin:\n",
    "    for line in tqdm(fin):\n",
    "        spls = preprocess_txt(line)\n",
    "        sentences.append(spls)\n",
    "        c += 1\n",
    "        if c > 500000:\n",
    "            break\n",
    "\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(DATA_PATH + 'sentences.pickle', 'wb') as f: # Save it for future use\n",
    "    pickle.dump(sentences, f)            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Подгружаем предварительно подготовленный датасет вопросов и ответов для обучения Word2Vec\n",
    "with open(DATA_PATH + 'sentences.pickle', 'rb') as f:\n",
    "    sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [i for i in sentences if len(i) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed, s: 68.04693550799857\n"
     ]
    }
   ],
   "source": [
    "# Обучим модель word2vec на наших вопросах\n",
    "t = time.perf_counter()\n",
    "model = Word2Vec(sentences=sentences, vector_size=100, min_count=1, window=5)\n",
    "\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Сохраняем обученную модель вопросов из болталки\n",
    "model.save(DATA_PATH + \"w2v_model\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Подгружаем предварительно обученную модель\n",
    "model = Word2Vec.load(DATA_PATH + \"w2v_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Индексация вопросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f64b81cf4474e1b91f2826961285af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed, s: 6318.924998086\n"
     ]
    }
   ],
   "source": [
    "# Складываем в индекс все наши вопросы из болталки\n",
    "t = time.perf_counter()\n",
    "index_s = annoy.AnnoyIndex(100, 'angular')\n",
    "\n",
    "index_map_s = {}\n",
    "counter = 0\n",
    "\n",
    "with codecs.open(DATA_PATH + \"prepared_answers.txt\", \"r\", \"utf_8_sig\") as f:\n",
    "\n",
    "    for line in tqdm(f):\n",
    "        n_w2v = 0\n",
    "        spls = line.split(\"\\t\")\n",
    "        if len(spls) > 1:\n",
    "            index_map_s[counter] = spls[1]\n",
    "            question = preprocess_txt(spls[0])\n",
    "            vector = np.zeros(100)\n",
    "            for word in question:\n",
    "                if word in model.wv:\n",
    "                    vector += model.wv[word]\n",
    "                    n_w2v += 1\n",
    "            if n_w2v > 0:\n",
    "                vector = vector / n_w2v\n",
    "            index_s.add_item(counter, vector)\n",
    "\n",
    "        counter += 1\n",
    "index_s.build(10)\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Сохраняем индекс вопросов из болталки\n",
    "with open(DATA_PATH + 'index_map_speaker.pickle', 'wb') as f:  # Save it for future use\n",
    "    pickle.dump(index_map_s, f)\n",
    "index_s.save(DATA_PATH + 'speaker.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_answer(question):\n",
    "    preprocessed_question = preprocess_txt(question)\n",
    "    n_w2v = 0\n",
    "    vector = np.zeros(100)\n",
    "    for word in preprocessed_question:\n",
    "        if word in model.wv:\n",
    "            vector += model.wv[word]\n",
    "            n_w2v += 1\n",
    "    if n_w2v > 0:\n",
    "        vector = vector / n_w2v\n",
    "    answer_index = index_s.get_nns_by_vector(vector, 1)\n",
    "    return index_map_s[answer_index[0]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Подгружаем предварительно подготовленный и сохранненый индекс\n",
    "with open(DATA_PATH + 'index_map_speaker.pickle', 'rb') as f:\n",
    "    index_map_s = pickle.load(f)\n",
    "index_s = annoy.AnnoyIndex(100, 'angular')    \n",
    "index_s.load(DATA_PATH + 'speaker.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ЭБУ — электронный блок управления двигателем автомобиля, его другое название — контроллер. Он принимает информацию от многочисленных датчиков, обрабатывает ее по особым алгоритмам и, отталкиваясь от полученных данных, отдает команды исполнительным устройствам системы.. \\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_answer('Эбу в двенашке называется Итэлма что за эбу?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Run Time Elapsed, s: 11613.777325656\n"
     ]
    }
   ],
   "source": [
    "print('Total Run Time Elapsed, s:', time.perf_counter() - tbeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обертка функций и реализация чат-бота"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Подгружаем ранее сохраненный и обученный векторайзер\n",
    "    with open(DATA_PATH + 'TfidfVectorizer.pickle', 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    # Подгружаем ранее сохраненный и обученный классификатор\n",
    "    with open(DATA_PATH + 'query_model_stem_tdfv_best.pickle', 'rb') as f:\n",
    "        model_svm = pickle.load(f)\n",
    "\n",
    "    # Подгружаем предварительно обученную модель продуктовых названий\n",
    "    model_w2v_products = Word2Vec.load(DATA_PATH + \"w2v_model_products\")\n",
    "\n",
    "    # Подгружаем предварительно подготовленный и сохранненый индекс продуктовых названий\n",
    "    with open(DATA_PATH + 'index_map_products.pickle', 'rb') as f:\n",
    "        index_map = pickle.load(f)\n",
    "    index = annoy.AnnoyIndex(100, 'angular')\n",
    "    index.load(DATA_PATH + 'products.ann')\n",
    "\n",
    "    # Подгружаем предварительно обученную модель вопросов из болталки\n",
    "    model = Word2Vec.load(DATA_PATH + \"w2v_model\")\n",
    "\n",
    "    # Подгружаем предварительно подготовленный и сохранненый индекс ответов из болталки\n",
    "    with open(DATA_PATH + 'index_map_speaker.pickle', 'rb') as f:\n",
    "        index_map_s = pickle.load(f)\n",
    "    index_s = annoy.AnnoyIndex(100, 'angular')\n",
    "    index_s.load(DATA_PATH + 'speaker.ann')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "r = RussianStemmer()\n",
    "morpher = MorphAnalyzer()\n",
    "sw_ = set(get_stop_words(\"ru\"))\n",
    "sw = sw_.union(\n",
    "    set('1234567890qwertyuiopasdfghjklzxcvbnmйцукенгшщзхъфываепнроглшщджэячсмитьбю'))\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "def get_answer(text):\n",
    "    \n",
    "    def pprocess(line):\n",
    "        # используется для обработки текстовых данных при обучении классификатора\n",
    "        line = re.sub(\"[\\x0b\\x0c\\s]\", ' ', line.lower())\n",
    "        line = re.sub(\"(б\\.у)|(б\\/у)|(б\\\\у)\", 'бу', line)\n",
    "        line = re.sub(\"[^а-яё \\d\\w]\", ' ', line)\n",
    "        spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "        spls = [r.stem(i) for i in spls]\n",
    "        spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "        return ' '.join(spls)\n",
    "\n",
    "    def products_preprocess_txt(line):\n",
    "        # используется для обработки текстовых данных при сворачивании продуктовых названий в  Word2Vec \n",
    "        line = re.sub(\"[\\x0b\\x0c\\s]\", ' ', line.lower())\n",
    "        line = re.sub(\"(б\\.у)|(б\\/у)|(б\\\\у)\", 'бу', line)\n",
    "        line = re.sub(\"[^а-яё \\d\\w]\", ' ', line)\n",
    "        spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "        spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "        spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "        return spls\n",
    "\n",
    "    def preprocess_txt(line):\n",
    "        # используется для обработки текстовых данных при сворачивании вопросов из болталки в  Word2Vec \n",
    "        spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "        spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "        spls = [i for i in spls if i not in sw_ and i != \"\"]\n",
    "        return spls\n",
    "\n",
    "    def find_description(product_title):\n",
    "        # находит id и title входному продуктовому запросу\n",
    "        preprocessed_title = products_preprocess_txt(product_title)\n",
    "        n_w2v = 0\n",
    "        vector = np.zeros(100)\n",
    "        for word in preprocessed_title:\n",
    "            if word in model_w2v_products.wv:\n",
    "                vector += model_w2v_products.wv[word]\n",
    "                n_w2v += 1\n",
    "        if n_w2v > 0:\n",
    "            vector = vector / n_w2v\n",
    "        answer_index = index.get_nns_by_vector(vector, 1)\n",
    "        return index_map[answer_index[0]]\n",
    "\n",
    "    def find_answer(question):\n",
    "        # находит ответ из болталки по  входному вопросу \n",
    "        preprocessed_question = preprocess_txt(question)\n",
    "        n_w2v = 0\n",
    "        vector = np.zeros(100)\n",
    "        for word in preprocessed_question:\n",
    "            if word in model.wv:\n",
    "                vector += model.wv[word]\n",
    "                n_w2v += 1\n",
    "        if n_w2v > 0:\n",
    "            vector = vector / n_w2v\n",
    "        answer_index = index_s.get_nns_by_vector(vector, 1)\n",
    "        return index_map_s[answer_index[0]]\n",
    "\n",
    "    # product or speaker?\n",
    "    if model_svm.predict(vectorizer.transform([pprocess(text)]))[0] == 1:\n",
    "        # speaker\n",
    "        return find_answer(text)\n",
    "    else:\n",
    "        # product\n",
    "        return find_description(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Автотесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_answer('Юбка детская ORBY').startswith('58e3cfe6132ca50e053f5f82'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(not get_answer('Где ключи от танка').startswith('5'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Список сохраненных файлов используемых в ноутбуке:\n",
    "# 'prodDS.csv'\n",
    "# 'ml_14_classify.csv'\n",
    "# 'ml_14_classify_pprocessed.csv'\n",
    "# 'TfidfVectorizer.pickle' - vectorizer for SVM classifier\n",
    "# 'query_model_stem_tdfv_best.pickle' - SVM classifier\n",
    "# 'w2v_model' - word2vec model for speaker questions\n",
    "# 'speaker.ann' - index for speaker questions\n",
    "# 'index_map_speaker.pickle' - index map for speaker questions\n",
    "# 'w2v_model_products' - word2vec model for product queries\n",
    "# 'products.ann' - index for product queries\n",
    "# 'index_map_products.pickle' - index map for product queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
